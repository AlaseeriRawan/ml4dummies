{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Opinions and Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding How Machines Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'The quick brown fox jumps over the lazy dog.'\n",
    "text_2 = 'My dog is quick and can jump over fences.'\n",
    "text_3 = 'Your dog is so lazy that it sleeps all the day.'\n",
    "corpus = [text_1, text_2, text_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer(binary=True).fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 19, 'quick': 15, 'brown': 2, 'fox': 7, 'jumps': 11, 'over': 14, 'lazy': 12, 'dog': 5, 'my': 13, 'is': 8, 'and': 1, 'can': 3, 'jump': 10, 'fences': 6, 'your': 20, 'so': 17, 'that': 18, 'it': 9, 'sleeps': 16, 'all': 0, 'day': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Enhancing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 0 0 2 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text_4 = 'A black dog just passed by but my dog is brown.'\n",
    "corpus.append(text_4)\n",
    "vectorizer = text.CountVectorizer().fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     brown: 0.095\n",
      "       dog: 0.126\n",
      "        my: 0.095\n",
      "        is: 0.077\n",
      "     black: 0.121\n",
      "      just: 0.121\n",
      "    passed: 0.121\n",
      "        by: 0.121\n",
      "       but: 0.121\n",
      "\n",
      "Summed values of a phrase: 1.0\n"
     ]
    }
   ],
   "source": [
    "TfidF = text.TfidfTransformer(norm='l1')\n",
    "tfidf_mtx = TfidF.fit_transform(vectorized_text)\n",
    "\n",
    "phrase = 3 # choose a number from 0 to 3\n",
    "\n",
    "total = 0\n",
    "for word in vectorizer.vocabulary_:\n",
    "    pos = vectorizer.vocabulary_[word]\n",
    "    value = list(tfidf_mtx.toarray()[phrase])[pos]\n",
    "    if value !=0:\n",
    "        print (\"%10s: %0.3f\" % (word, value))\n",
    "        total += value\n",
    "print('\\nSummed values of a phrase: %0.1f' % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the quick': 30, 'quick brown': 24, 'brown fox': 3, 'fox jumps': 9, 'jumps over': 15, 'over the': 21, 'the lazy': 29, 'lazy dog': 17, 'my dog': 19, 'dog is': 7, 'is quick': 11, 'quick and': 23, 'and can': 1, 'can jump': 6, 'jump over': 14, 'over fences': 20, 'your dog': 31, 'is so': 12, 'so lazy': 26, 'lazy that': 18, 'that it': 27, 'it sleeps': 13, 'sleeps all': 25, 'all the': 0, 'the day': 28, 'black dog': 2, 'dog just': 8, 'just passed': 16, 'passed by': 22, 'by but': 5, 'but my': 4, 'is brown': 10}\n"
     ]
    }
   ],
   "source": [
    "bigrams = text.CountVectorizer(ngram_range=(2,2))\n",
    "print(bigrams.fit(corpus).vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'sam', 'swim', 'time']\n",
      "[[1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokenizing\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stemming\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vocab = ['Sam loves swimming so he swims all the time']\n",
    "vect = text.CountVectorizer(tokenizer=tokenize)\n",
    "vec = vect.fit(vocab)\n",
    "\n",
    "sentence1 = vec.transform(['George loves swimming too!'])\n",
    "\n",
    "print(vec.get_feature_names())\n",
    "print(sentence1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Textual Datasets from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "try:\n",
    "    import urllib2 # Python 2.7.x\n",
    "except:\n",
    "    import urllib.request as urllib2 # Python 3.x\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "header = {'User-Agent': 'Mozilla/5.0'} \n",
    "query = urllib2.Request(wiki, headers=header)\n",
    "page = urllib2.urlopen(query)\n",
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", { \"class\" : \"wikitable sortable\" })\n",
    "final_table = list()\n",
    "\n",
    "def extract_txt(cell):\n",
    "    \"\"\"Extracting only text\"\"\"\n",
    "    cells = [c.strip() for c in cell.findAll(text=True) if '[' not in c]\n",
    "    return ' '.join(cells).strip()\n",
    "\n",
    "def filter_sq(txt):\n",
    "    \"\"\"Extracting squared meter values\"\"\"\n",
    "    return txt.split('sq')[0].strip()\n",
    "\n",
    "cols = [extract_txt(cell) for cell in table.findAll(\"th\")]\n",
    "columns = [cols[1], cols[2], cols[3], cols[4], cols[6]]\n",
    "\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll(\"td\")\n",
    "    if len(cells)>0:\n",
    "        final_table.append([extract_txt(cells[1]), \n",
    "                            extract_txt(cells[2]), \n",
    "                            extract_txt(cells[3]), \n",
    "                            extract_txt(cells[4]), \n",
    "                            filter_sq(extract_txt(cells[6]))\n",
    "                           ])\n",
    "        \n",
    "df = pd.DataFrame(final_table, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>2019 estimate</th>\n",
       "      <th>2010 Census</th>\n",
       "      <th>2016 land area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>8,336,817</td>\n",
       "      <td>8,175,133</td>\n",
       "      <td>301.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>3,979,576</td>\n",
       "      <td>3,792,621</td>\n",
       "      <td>468.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>2,693,976</td>\n",
       "      <td>2,695,598</td>\n",
       "      <td>227.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Houston</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2,320,268</td>\n",
       "      <td>2,100,263</td>\n",
       "      <td>637.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1,680,992</td>\n",
       "      <td>1,445,632</td>\n",
       "      <td>517.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          City       State 2019 estimate 2010 Census 2016 land area\n",
       "0     New York    New York     8,336,817   8,175,133          301.5\n",
       "1  Los Angeles  California     3,979,576   3,792,621          468.7\n",
       "2      Chicago    Illinois     2,693,976   2,695,598          227.3\n",
       "3      Houston       Texas     2,320,268   2,100,263          637.5\n",
       "4      Phoenix     Arizona     1,680,992   1,445,632          517.6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scoring and Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'https://github.com/lmassaron/datasets/releases/download/1.0/'\n",
    "filename += 'shakespeare_lines_in_plays.feather'\n",
    "shakespeare = pd.read_feather(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=3, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(shakespeare.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=999,\n",
       "    n_components=10, random_state=101, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=n_topics, max_iter=999, random_state=101)\n",
    "nmf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvectorizer = CountVectorizer(max_df=1.0, min_df=2, stop_words='english')\n",
    "counts = cvectorizer.fit_transform(shakespeare.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=30,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=101, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                max_iter=30,\n",
    "                                learning_method='batch',\n",
    "                                random_state=101)\n",
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0: thou thy thee love ll shall come art let sir\n",
      "Topic # 1: enter exeunt exit scene ii iii iv palace attendants act\n",
      "Topic # 2: king thy lord warwick henry edward france york shall richard\n",
      "Topic # 3: caesar antony brutus cassius shall rome cleopatra egypt casca thou\n",
      "Topic # 4: antipholus dromio chain sir syracuse ephesus thou dinner home master\n",
      "Topic # 5: page ford sir master mistress shall good anne come kate\n",
      "Topic # 6: rome marcius titus lavinia andronicus marcus lucius coriolanus thy tribunes\n",
      "Topic # 7: lord good shall sir know ll man come let love\n",
      "Topic # 8: cassio iago desdemona othello moor roderigo emilia handkerchief lieutenant cyprus\n",
      "Topic # 9: hector troilus achilles ajax troy agamemnon diomed cressid patroclus pandarus\n"
     ]
    }
   ],
   "source": [
    "def print_topic_words(features, topics, top=10):\n",
    "    for idx, topic in enumerate(topics):\n",
    "        words = \" \".join([features[i] \n",
    "                for i in topic.argsort()[:(-top-1):-1]])\n",
    "        print(f\"Topic #{idx:2.0f}: {words}\")\n",
    "\n",
    "print_topic_words(vectorizer.get_feature_names(), nmf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Othello act:5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "index = shakespeare.play + ' act:' + shakespeare.act\n",
    "\n",
    "def find_top_match(model, data, index, topic_no):\n",
    "    topic_scores = model.transform(data)[:, topic_no]\n",
    "    best_score = np.argmax(topic_scores)\n",
    "    print(index[best_score])\n",
    "\n",
    "find_top_match(nmf, tfidf, index, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0: enter exeunt exit scene ii iii act iv palace attendants\n",
      "Topic # 1: king lord god york duke henry richard england edward warwick\n",
      "Topic # 2: sir love thou shall good come ll man thee let\n",
      "Topic # 3: king gloucester talbot france henry edmund french kent arthur hubert\n",
      "Topic # 4: antipholus petruchio katharina dromio gremio kate chain abbey abbess aegeon\n",
      "Topic # 5: thou thy thee shall lord ll good come let like\n",
      "Topic # 6: tyre antiochus griffith angelo monster antioch pericles ariel claudio grace\n",
      "Topic # 7: shall good antipholus king know let great sir hath lord\n",
      "Topic # 8: rome titus marcus thy lavinia emperor lucius lord andronicus sons\n",
      "Topic # 9: caesar antony brutus master page mistress sir ford mark cassius\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(cvectorizer.get_feature_names(), lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Gentlemen of Verona act:2\n"
     ]
    }
   ],
   "source": [
    "find_top_match(lda, counts, index, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing reviews from e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'https://github.com/lmassaron/datasets/releases/download/1.0/'\n",
    "filename += 'imdb_50k.feather'\n",
    "reviews = pd.read_feather(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews.sample(30000, random_state=42)\n",
    "validation = reviews[~reviews.index.isin(train.index)].sample(10000, random_state=42)\n",
    "test = reviews[~reviews.index.isin(train.index.append(validation.index) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.7% of reviews are 2581 or less characters\n"
     ]
    }
   ],
   "source": [
    "pct_90 = reviews.review.apply(len).quantile(0.90)\n",
    "print(f\"98.7% of reviews are {pct_90:0.0f} or less characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(tokenizer.texts_to_sequences(train.review), maxlen=256)\n",
    "y = train.sentiment.values\n",
    "Xv = pad_sequences(tokenizer.texts_to_sequences(validation.review), maxlen=256)\n",
    "yv = validation.sentiment.values\n",
    "Xt = pad_sequences(tokenizer.texts_to_sequences(test.review), maxlen=256)\n",
    "yt = test.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 256, 8)            790040    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 792,089\n",
      "Trainable params: 792,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(len(tokenizer.index_word)+1, 8, input_length=256))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.4363 - acc: 0.7912 - val_loss: 0.2781 - val_acc: 0.8886\n",
      "Epoch 2/2\n",
      "30000/30000 [==============================] - 44s 1ms/sample - loss: 0.1907 - acc: 0.9290 - val_loss: 0.2587 - val_acc: 0.8969\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=2,batch_size=16, \n",
    "                    validation_data=(Xv, yv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      5024\n",
      "           1       0.91      0.87      0.89      4976\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.89      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = (model.predict(Xt) > 0.5).astype(int)\n",
    "print(classification_report(yt, predictions))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
